{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_neo4j import GraphCypherQAChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "import vansy_f8a_genai_test.report_a03_rag_and_reasoning_frameworks_tutorial_prompt as prompt_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lg/yb401ry14w19vfs73n7y2rtw0000gn/T/ipykernel_66474/88119791.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    k=5,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital Cypher Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lg/yb401ry14w19vfs73n7y2rtw0000gn/T/ipykernel_66474/249957495.py:1: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
      "  graph = Neo4jGraph(\n"
     ]
    }
   ],
   "source": [
    "graph = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URI\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    ")\n",
    "\n",
    "graph.refresh_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_generation_template = prompt_hub.CYPHER_GENERATION_TEMPLATE\n",
    "qa_generation_template = prompt_hub.QA_GENERATION_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=cypher_generation_template\n",
    ")\n",
    "\n",
    "qa_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=qa_generation_template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "    qa_llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    qa_prompt=qa_generation_prompt,\n",
    "    cypher_prompt=cypher_generation_prompt,\n",
    "    validate_cypher=True,\n",
    "    top_k=100,\n",
    "    allow_dangerous_requests=True,\n",
    "    # use_function_response=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital Review Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_vector_index = Neo4jVector.from_existing_graph(\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "    url=os.getenv(\"NEO4J_URI\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    "    index_name=\"reviews\",\n",
    "    node_label=\"Review\",\n",
    "    text_node_properties=[\n",
    "        \"physician_name\",\n",
    "        \"patient_name\",\n",
    "        \"text\",\n",
    "        \"hospital_name\",\n",
    "    ],\n",
    "    embedding_node_property=\"embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_generation_template = prompt_hub.REVIEW_GENERATION_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"context\"], template=review_generation_template\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_human_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(input_variables=[\"question\"], template=\"{question}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [review_system_prompt, review_human_prompt]\n",
    "review_prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_vector_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=neo4j_vector_index.as_retriever(k=12),\n",
    ")\n",
    "reviews_vector_chain.combine_documents_chain.llm_chain.prompt = review_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent, Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the tools for agent can use\n",
    "tools = [\n",
    "    Tool(\n",
    "    name=\"HospitalGraphQA\",\n",
    "    # The 'func' is the chain's invoke method. We use a lambda to map the\n",
    "    # string input from the agent to the dictionary input the chain expects.\n",
    "    func=lambda q: hospital_cypher_chain.invoke({\"query\": q}),\n",
    "    description=\"\"\"Useful for answering questions about patients,\n",
    "        physicians, hospitals, insurance payers, patient review\n",
    "        statistics, and hospital visit details. Use the entire prompt as\n",
    "        input to the tool. For instance, if the prompt is 'How many visits\n",
    "        have there been?\", the input should be \"How many visits have\n",
    "        there been?'\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "    name=\"HospitalReviewsQA\",\n",
    "    func=lambda q: reviews_vector_chain.invoke({\"query\": q}),\n",
    "    description=\"\"\"Useful when you need to answer questions\n",
    "        about patient experiences, feelings, or any other qualitative\n",
    "        question that could be answered about a patient using semantic\n",
    "        search. Not useful for answering objective questions that involve\n",
    "        counting, percentages, aggregations, or listing facts. Use the\n",
    "        entire prompt as input to the tool. For instance, if the prompt is\n",
    "        \"Are patients satisfied with their care?\", the input should be\n",
    "        \"Are patients satisfied with their care?\".\n",
    "        \"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vansy/Documents/Projects/vansy-f8a-genai-test/.venv/lib/python3.13/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 2. Set up the ReAct Agent\n",
    "# Pull the standard ReAct prompt\n",
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'input_variables': ['agent_scratchpad', 'input', 'tool_names', 'tools'],\n",
       " 'optional_variables': [],\n",
       " 'output_parser': None,\n",
       " 'partial_variables': {},\n",
       " 'metadata': {'lc_hub_owner': 'hwchase17',\n",
       "  'lc_hub_repo': 'react',\n",
       "  'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'},\n",
       " 'tags': None,\n",
       " 'template': 'Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}',\n",
       " 'template_format': 'f-string',\n",
       " 'validate_template': False,\n",
       " '_type': 'prompt'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt_template = \"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "PREVIOUS CHAT HISTORY:\n",
    "{chat_history}\n",
    "\n",
    "NEW QUESTION:\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=['chat_history','agent_scratchpad', 'input', 'tool_names', 'tools'],\n",
    "    template=custom_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM for the agent to use\n",
    "# This can be the same or different from the models in your chain\n",
    "agent_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = create_react_agent(agent_llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM for the agent to use\n",
    "# This can be the same or different from the models in your chain\n",
    "agent_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM for the agent to use\n",
    "# This can be the same or different from the models in your chain\n",
    "agent_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM for the agent to use\n",
    "# This can be the same or different from the models in your chain\n",
    "agent_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent executor to run the agent\n",
    "agent_executor = AgentExecutor(agent=agent, \n",
    "                               tools=tools, verbose=True, \n",
    "                               memory=memory, \n",
    "                               return_intermediate_steps=True,\n",
    "                               handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with the Hospital AI Agent! Type 'exit' to quit.\n",
      "\n",
      "User: how many hospital in Florida?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find out the number of hospitals in Florida. Since this is an objective question that involves counting, I will use the HospitalGraphQA tool to get the information.\n",
      "\n",
      "Action: HospitalGraphQA  \n",
      "Action Input: how many hospital in Florida?  \u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (h:Hospital)\n",
      "WHERE h.state_name = 'FL'\n",
      "RETURN COUNT(h) AS hospital_count\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'hospital_count': 7}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m{'query': 'how many hospital in Florida?', 'result': 'There are 7 hospitals in Florida.'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vansy/Documents/Projects/vansy-f8a-genai-test/.venv/lib/python3.13/site-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferWindowMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: There are 7 hospitals in Florida.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Agent: There are 7 hospitals in Florida.\n",
      "\n",
      "User: can you name them?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find out the names of the hospitals in Florida. Since this is an objective question that involves listing facts, I will use the HospitalGraphQA tool to retrieve this information.\n",
      "\n",
      "Action: HospitalGraphQA  \n",
      "Action Input: \"Can you name the hospitals in Florida?\"  \u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (h:Hospital)\n",
      "WHERE h.state_name = 'FL'\n",
      "RETURN h.name AS hospital_name\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'hospital_name': 'Walton LLC'}, {'hospital_name': 'Wheeler, Bryant and Johns'}, {'hospital_name': 'Brown Inc'}, {'hospital_name': 'Smith, Edwards and Obrien'}, {'hospital_name': 'Rose Inc'}, {'hospital_name': 'Jordan Inc'}, {'hospital_name': 'Lewis-Nelson'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m{'query': 'Can you name the hospitals in Florida?', 'result': 'The hospitals in Florida are as follows:\\n\\n1. Walton LLC\\n2. Wheeler, Bryant and Johns\\n3. Brown Inc\\n4. Smith, Edwards and Obrien\\n5. Rose Inc\\n6. Jordan Inc\\n7. Lewis-Nelson\\n\\nIf you need more information about any of these hospitals, feel free to ask!'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vansy/Documents/Projects/vansy-f8a-genai-test/.venv/lib/python3.13/site-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferWindowMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: The hospitals in Florida are as follows:  \n",
      "1. Walton LLC  \n",
      "2. Wheeler, Bryant and Johns  \n",
      "3. Brown Inc  \n",
      "4. Smith, Edwards and Obrien  \n",
      "5. Rose Inc  \n",
      "6. Jordan Inc  \n",
      "7. Lewis-Nelson  \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Agent: The hospitals in Florida are as follows:  \n",
      "1. Walton LLC  \n",
      "2. Wheeler, Bryant and Johns  \n",
      "3. Brown Inc  \n",
      "4. Smith, Edwards and Obrien  \n",
      "5. Rose Inc  \n",
      "6. Jordan Inc  \n",
      "7. Lewis-Nelson\n",
      "\n",
      "User: exit\n",
      "\n",
      "Agent: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# --- 3. The Interactive Chat Loop ---\n",
    "print(\"Chat with the Hospital AI Agent! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    try:\n",
    "        user_question = input(\"You: \")\n",
    "        print(f\"\\nUser: {user_question}\")\n",
    "        if user_question.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"\\nAgent: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Invoke the agent executor with the user's question\n",
    "        response = agent_executor.invoke({\"input\": user_question})\n",
    "        \n",
    "        # Print the final answer\n",
    "        print(f\"\\nAgent: {response['output']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find out the number of hospitals in Colorado (CO). Since this is an objective question that involves counting, I will use the HospitalGraphQA tool to get the information.\n",
      "\n",
      "Action: HospitalGraphQA  \n",
      "Action Input: How many hospital in CO?  \u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (h:Hospital)\n",
      "WHERE h.state_name = 'CO'\n",
      "RETURN COUNT(h) AS hospital_count\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'hospital_count': 8}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m{'query': 'How many hospital in CO?', 'result': 'There are 8 hospitals in Colorado.'}\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: There are 8 hospitals in Colorado.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vansy/Documents/Projects/vansy-f8a-genai-test/.venv/lib/python3.13/site-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferWindowMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response = agent_executor.invoke({\"input\": \"How many hospital in CO?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How many hospital in CO?',\n",
       " 'chat_history': [HumanMessage(content='How many hospital in LA?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='There are currently no hospitals in LA.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='how many hospital in Florida?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='There are 7 hospitals in Florida.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='can you name them?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The hospitals in Florida are as follows:  \\n1. Walton LLC  \\n2. Wheeler, Bryant and Johns  \\n3. Brown Inc  \\n4. Smith, Edwards and Obrien  \\n5. Rose Inc  \\n6. Jordan Inc  \\n7. Lewis-Nelson', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How many hospital in Florida?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='There are 7 hospitals in Florida.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='How many hospital in Florida?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='There are 7 hospitals in Florida.', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'There are 8 hospitals in Colorado.',\n",
       " 'intermediate_steps': [(AgentAction(tool='HospitalGraphQA', tool_input='How many hospital in CO?', log='I need to find out the number of hospitals in Colorado (CO). Since this is an objective question that involves counting, I will use the HospitalGraphQA tool to get the information.\\n\\nAction: HospitalGraphQA  \\nAction Input: How many hospital in CO?  '),\n",
       "   {'query': 'How many hospital in CO?',\n",
       "    'result': 'There are 8 hospitals in Colorado.'})]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00015314999999999998, 1446)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.total_cost, cb.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentAction(tool='HospitalGraphQA', tool_input='How many hospital in CO?', log='I need to find out the number of hospitals in Colorado (CO). Since this is an objective question that involves counting, I will use the HospitalGraphQA tool to get the information.\\n\\nAction: HospitalGraphQA  \\nAction Input: How many hospital in CO?  ')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = response['intermediate_steps'][0][0]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HospitalGraphQA'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
