{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_neo4j import GraphCypherQAChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "import vansy_f8a_genai_test.report_a03_rag_and_reasoning_frameworks_tutorial_prompt as prompt_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lg/yb401ry14w19vfs73n7y2rtw0000gn/T/ipykernel_66474/88119791.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    k=5,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital Cypher Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lg/yb401ry14w19vfs73n7y2rtw0000gn/T/ipykernel_66474/249957495.py:1: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
      "  graph = Neo4jGraph(\n"
     ]
    }
   ],
   "source": [
    "graph = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URI\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    ")\n",
    "\n",
    "graph.refresh_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_generation_template = prompt_hub.CYPHER_GENERATION_TEMPLATE\n",
    "qa_generation_template = prompt_hub.QA_GENERATION_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=cypher_generation_template\n",
    ")\n",
    "\n",
    "qa_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=qa_generation_template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "    qa_llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    qa_prompt=qa_generation_prompt,\n",
    "    cypher_prompt=cypher_generation_prompt,\n",
    "    validate_cypher=True,\n",
    "    top_k=100,\n",
    "    allow_dangerous_requests=True,\n",
    "    # use_function_response=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital Review Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_vector_index = Neo4jVector.from_existing_graph(\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "    url=os.getenv(\"NEO4J_URI\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    "    index_name=\"reviews\",\n",
    "    node_label=\"Review\",\n",
    "    text_node_properties=[\n",
    "        \"physician_name\",\n",
    "        \"patient_name\",\n",
    "        \"text\",\n",
    "        \"hospital_name\",\n",
    "    ],\n",
    "    embedding_node_property=\"embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_generation_template = prompt_hub.REVIEW_GENERATION_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"context\"], template=review_generation_template\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_human_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(input_variables=[\"question\"], template=\"{question}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [review_system_prompt, review_human_prompt]\n",
    "review_prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_vector_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=neo4j_vector_index.as_retriever(k=12),\n",
    ")\n",
    "reviews_vector_chain.combine_documents_chain.llm_chain.prompt = review_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent, Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the tools for agent can use\n",
    "tools = [\n",
    "    Tool(\n",
    "    name=\"HospitalGraphQA\",\n",
    "    # The 'func' is the chain's invoke method. We use a lambda to map the\n",
    "    # string input from the agent to the dictionary input the chain expects.\n",
    "    func=lambda q: hospital_cypher_chain.invoke({\"query\": q}),\n",
    "    description=\"\"\"Useful for answering questions about patients,\n",
    "        physicians, hospitals, insurance payers, patient review\n",
    "        statistics, and hospital visit details. Use the entire prompt as\n",
    "        input to the tool. For instance, if the prompt is 'How many visits\n",
    "        have there been?\", the input should be \"How many visits have\n",
    "        there been?'\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "    name=\"HospitalReviewsQA\",\n",
    "    func=lambda q: reviews_vector_chain.invoke({\"query\": q}),\n",
    "    description=\"\"\"Useful when you need to answer questions\n",
    "        about patient experiences, feelings, or any other qualitative\n",
    "        question that could be answered about a patient using semantic\n",
    "        search. Not useful for answering objective questions that involve\n",
    "        counting, percentages, aggregations, or listing facts. Use the\n",
    "        entire prompt as input to the tool. For instance, if the prompt is\n",
    "        \"Are patients satisfied with their care?\", the input should be\n",
    "        \"Are patients satisfied with their care?\".\n",
    "        \"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vansy/Documents/Projects/vansy-f8a-genai-test/.venv/lib/python3.13/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 2. Set up the ReAct Agent\n",
    "# Pull the standard ReAct prompt\n",
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'input_variables': ['agent_scratchpad', 'input', 'tool_names', 'tools'],\n",
       " 'optional_variables': [],\n",
       " 'output_parser': None,\n",
       " 'partial_variables': {},\n",
       " 'metadata': {'lc_hub_owner': 'hwchase17',\n",
       "  'lc_hub_repo': 'react',\n",
       "  'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'},\n",
       " 'tags': None,\n",
       " 'template': 'Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}',\n",
       " 'template_format': 'f-string',\n",
       " 'validate_template': False,\n",
       " '_type': 'prompt'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt_template = \"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "PREVIOUS CHAT HISTORY:\n",
    "{chat_history}\n",
    "\n",
    "NEW QUESTION:\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=['chat_history','agent_scratchpad', 'input', 'tool_names', 'tools'],\n",
    "    template=custom_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM for the agent to use\n",
    "# This can be the same or different from the models in your chain\n",
    "agent_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = create_react_agent(agent_llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM for the agent to use\n",
    "# This can be the same or different from the models in your chain\n",
    "agent_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM for the agent to use\n",
    "# This can be the same or different from the models in your chain\n",
    "agent_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM for the agent to use\n",
    "# This can be the same or different from the models in your chain\n",
    "agent_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent executor to run the agent\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with the Hospital AI Agent! Type 'exit' to quit.\n",
      "\n",
      "User: How many visits were there at 'Jones, Brown and Murray'?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo find out the number of visits at 'Jones, Brown and Murray', I will use the HospitalGraphQA tool, as it is designed to provide objective data regarding hospital visits.\n",
      "\n",
      "Action: HospitalGraphQA  \n",
      "Action Input: How many visits were there at 'Jones, Brown and Murray'?  \u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (h:Hospital {name: 'Jones, Brown and Murray'})<-[:AT]-(v:Visit)\n",
      "RETURN COUNT(v) AS visit_count\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'visit_count': 317}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m{'query': \"How many visits were there at 'Jones, Brown and Murray'?\", 'result': \"There were a total of 317 visits at 'Jones, Brown and Murray'.\"}\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: There were a total of 317 visits at 'Jones, Brown and Murray'.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Agent: There were a total of 317 visits at 'Jones, Brown and Murray'.\n",
      "\n",
      "User: What people think about that hospital?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to gather qualitative feedback about patient experiences and opinions regarding 'Jones, Brown and Murray' hospital. \n",
      "\n",
      "Action: HospitalReviewsQA  \n",
      "Action Input: What people think about that hospital?  \u001b[0m\u001b[33;1m\u001b[1;3m{'query': 'What people think about that hospital?', 'result': 'The reviews for Jones, Brown and Murray hospital indicate a mixed experience among patients. Christine Jones appreciated the warm and welcoming atmosphere of the hospital but noted that noise levels made it difficult to rest at times. Tara Murray also had a mixed experience; she found the medical care to be excellent but expressed concern about the lack of amenities for families, which made it challenging for her loved ones. Overall, while there are positive aspects regarding the atmosphere and medical care, there are also some drawbacks mentioned by patients.'}\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: The reviews for Jones, Brown and Murray hospital indicate a mixed experience among patients. Some appreciated the warm and welcoming atmosphere, while others noted issues like noise levels and a lack of amenities for families. Overall, there are positive aspects regarding the atmosphere and medical care, but also some drawbacks mentioned by patients.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Agent: The reviews for Jones, Brown and Murray hospital indicate a mixed experience among patients. Some appreciated the warm and welcoming atmosphere, while others noted issues like noise levels and a lack of amenities for families. Overall, there are positive aspects regarding the atmosphere and medical care, but also some drawbacks mentioned by patients.\n",
      "\n",
      "User: what hospital I asked?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe hospital you asked about is 'Jones, Brown and Murray'. \n",
      "\n",
      "Final Answer: The hospital you asked about is 'Jones, Brown and Murray'.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Agent: The hospital you asked about is 'Jones, Brown and Murray'.\n",
      "\n",
      "User: exit\n",
      "\n",
      "Agent: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# --- 3. The Interactive Chat Loop ---\n",
    "print(\"Chat with the Hospital AI Agent! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    try:\n",
    "        user_question = input(\"You: \")\n",
    "        print(f\"\\nUser: {user_question}\")\n",
    "        if user_question.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"\\nAgent: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Invoke the agent executor with the user's question\n",
    "        response = agent_executor.invoke({\"input\": user_question})\n",
    "        \n",
    "        # Print the final answer\n",
    "        print(f\"\\nAgent: {response['output']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
