{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_neo4j import GraphCypherQAChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import vansy_f8a_genai_test.report_a03_rag_and_reasoning_frameworks_tutorial_prompt as prompt_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital Cypher Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URI\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    ")\n",
    "\n",
    "graph.refresh_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_generation_template = prompt_hub.CYPHER_GENERATION_TEMPLATE\n",
    "qa_generation_template = prompt_hub.QA_GENERATION_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=cypher_generation_template\n",
    ")\n",
    "\n",
    "qa_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=qa_generation_template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "    qa_llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    qa_prompt=qa_generation_prompt,\n",
    "    cypher_prompt=cypher_generation_prompt,\n",
    "    validate_cypher=True,\n",
    "    top_k=100,\n",
    "    allow_dangerous_requests=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent, Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wrap your chain in the Tool class\n",
    "hospital_tool = Tool(\n",
    "    name=\"HospitalGraphQA\",\n",
    "    # The 'func' is the chain's invoke method. We use a lambda to map the\n",
    "    # string input from the agent to the dictionary input the chain expects.\n",
    "    func=lambda q: hospital_cypher_chain.invoke({\"query\": q}),\n",
    "    description=\"\"\"Useful for answering questions about patients,\n",
    "        physicians, hospitals, insurance payers, patient review\n",
    "        statistics, and hospital visit details. Use the entire prompt as\n",
    "        input to the tool. For instance, if the prompt is 'How many visits\n",
    "        have there been?\", the input should be \"How many visits have\n",
    "        there been?'\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the tools for agent can use\n",
    "tools = [hospital_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vansy/Documents/Projects/vansy-f8a-genai-test/.venv/lib/python3.13/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. Set up the ReAct Agent\n",
    "# Pull the standard ReAct prompt\n",
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'input_variables': ['agent_scratchpad', 'input', 'tool_names', 'tools'],\n",
       " 'optional_variables': [],\n",
       " 'output_parser': None,\n",
       " 'partial_variables': {},\n",
       " 'metadata': {'lc_hub_owner': 'hwchase17',\n",
       "  'lc_hub_repo': 'react',\n",
       "  'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'},\n",
       " 'tags': None,\n",
       " 'template': 'Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}',\n",
       " 'template_format': 'f-string',\n",
       " 'validate_template': False,\n",
       " '_type': 'prompt'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM for the agent to use\n",
    "# This can be the same or different from the models in your chain\n",
    "agent_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = create_react_agent(agent_llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent executor to run the agent\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with the Hospital AI Agent! Type 'exit' to quit.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find information about the oldest patient, which likely involves querying patient data to determine their ages. \n",
      "Action: HospitalGraphQA\n",
      "Action Input: \"Who is the oldest patient and how old are they?\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Patient)\n",
      "RETURN p.name AS oldest_patient,\n",
      "       duration.between(date(p.dob), date()).years AS age\n",
      "ORDER BY age DESC\n",
      "LIMIT 1\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'oldest_patient': 'Amanda Ortiz', 'age': 95}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m{'query': 'Who is the oldest patient and how old are they?', 'result': 'The oldest patient is Amanda Ortiz, who is 95 years old.'}\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: The oldest patient is Amanda Ortiz, who is 95 years old.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Agent: The oldest patient is Amanda Ortiz, who is 95 years old.\n",
      "\n",
      "Agent: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# --- 4. The Interactive Chat Loop ---\n",
    "print(\"Chat with the Hospital AI Agent! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    try:\n",
    "        user_question = input(\"You: \")\n",
    "        print(f\"User: {user_question}\")\n",
    "        if user_question.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"\\nAgent: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Invoke the agent executor with the user's question\n",
    "        response = agent_executor.invoke({\"input\": user_question})\n",
    "        \n",
    "        # Print the final answer\n",
    "        print(f\"\\nAgent: {response['output']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
